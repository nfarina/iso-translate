<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Gemini Live Voice to Text Realtime Stream</title>
    <!-- By Jim Salsman, April 2025. Released under the free MIT License. -->
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
      /* General body styles */
      body {
        font-family: sans-serif;
        display: flex;
        flex-direction: column;
        height: 100vh;
        margin: 0;
        overflow: hidden; /* Prevent body from scrolling */
        background-color: #eee;
      }

      /* Control bar styles */
      #controls {
        padding: 5px;
        /* Light background and border */
        background-color: lightcyan;
        border-bottom: 1px solid #ccc;
        text-align: center;
        position: relative;
      }
      #controls-content {
        display: flex;
        align-items: center;
        justify-content: flex-end;
        width: 100%;
        text-align: center; /* Center content horizontally */
        gap: 5px; /* Add gap between elements */
      }

      /* Start/Stop listening button */
      #toggleStream {
        padding: 10px 20px;
        font-size: 1em;
        cursor: pointer;
        background-color: #4c4;
        color: white;
        border: none;
        border-radius: 5px;
      }

      /* Send image button */
      #sendImage {
        background-color: blue;
        display: none; /* Initially hidden */
        border: none;
        color: white;
        font-size: 1em;
        padding: 10px 20px;
        border-radius: 5px;
      }

      /* Send text button */
      #sendText {
        background-color: darkorange;
        display: none; /* Initially hidden */
        border: none;
        color: white;
        font-size: 1em;
        padding: 10px 20px;
        border-radius: 5px;
      }

      /* Style for 'stop' state of the listening button */
      #toggleStream.stop {
        background-color: red;
      }

      /* Title style, floated to the left */
      #title {
        font-size: 1.2em;
        border-radius: 5px;
        margin-right: auto; /* Push the title to the far left */
      }

      /* Styles for debug input elements */
      #controls-content > label,
      #controls-content > input {
        margin-left: 5px;
      }

      /* Debug container style */
      #debugContainer {
        margin-right: auto; /* Push the container to the far left */
        text-align: center; /* Center the content */
      }

      /* Main output container styles */
      #output-container {
        flex-grow: 1; /* Take up remaining space */
        padding: 10px;
        padding-bottom: 30px;
        overflow-y: auto; /* Enable scrolling if content overflows */
        background-color: white;
        border: 1px solid lightgray;
        margin: 8px;
        border-radius: 5px;
        scroll-behavior: smooth; /* Smooth scroll behavior */
      }

      /* Output text style */
      #output {
        white-space: pre-wrap; /* Wrap long lines */
        word-wrap: break-word; /* Break long words onto multiple lines */
        font-family: sans-serif;
        font-size: 0.9em;
      }

      /* Styles to eliminate extra whitespace around output content */
      #output p {
        margin-bottom: -0.5em; /* Remove vertical whitespace */
        margin-top: -0.2em;
      }
      #output ul,
      #output ol,
      #output li {
        margin-top: -0.5em;
        margin-bottom: -0.5em; /* Remove vertical whitespace */
      }

      /* Styles to eliminate extra whitespace around <pre> output content */
      #output pre {
        margin: 0; /* Remove vertical whitespace */
        white-space: pre-wrap; /* Wrap long lines */
      }

      /* Styles for images in the output area */
      .output-image {
        max-height: 50vh;
        max-width: 90vw;
      }

      /* Output text styles */
      .error {
        color: red;
        font-weight: bold;
      }
      .info {
        color: blue;
        font-style: italic;
      }

      /* Styles for floating text input */
      #floatingTextInput {
        display: none; /* Initially hidden */
        position: fixed; /* Fixed position */
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%); /* Center */
        width: 80vw; /* 80% of viewport width */
        max-width: 600px; /* Max width */
        background-color: rgba(
          255,
          255,
          255,
          0.95
        ); /* Semi-transparent background */
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 20px rgba(0, 0, 0, 0.3);
        z-index: 100; /* Ensure it's on top */
      }
      #textInput {
        width: 100%;
        height: 10em; /* 10 lines */
        margin-bottom: 10px;
        resize: vertical; /* Allow vertical resize */
      }
    </style>
    <script
      src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
      crossorigin="anonymous"
    ></script>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
      crossorigin="anonymous"
    />
  </head>
  <body>
    <div id="controls">
      <div id="controls-content">
        <h1 id="title">Gemini Live<br />Voice to Text Stream</h1>
        <div id="debugContainer">
          <a
            href="https://github.com/jsalsman/gemini-live"
            target="_blank"
            style="text-decoration: none"
            >This code is on GitHub.</a
          >
          <br /><br />
          <input type="checkbox" id="debugCheckbox" />
          <label for="debugCheckbox"
            ><small
              >Debug<br />
              &nbsp;&nbsp;to console</small
            ></label
          >
        </div>
        <button id="toggleStream">
          Start<br />
          Listening
        </button>
        <button id="sendImage">
          Send<br />
          Image
        </button>
        <button id="sendText">
          Send<br />
          Text
        </button>
      </div>
    </div>

    <div id="output-container">
      <pre id="output"></pre>
    </div>

    <!-- Floating Text Input Area -->
    <div id="floatingTextInput">
      <textarea id="textInput"></textarea>
      <button>Send</button>
      <small>or Enter to send; use Shift+Enter for newlines</small>
    </div>

    <script type="module">
      // Import necessary modules
      import { marked } from "https://esm.sh/marked"; // Markdown
      import markedKatex from "https://esm.sh/marked-katex-extension"; // LaTeX
      import { GoogleGenAI, Modality } from "https://esm.run/@google/genai";

      // Docs: https://ai.google.dev/gemini-api/docs/live
      // https://googleapis.github.io/js-genai/main/index.html
      // https://github.com/googleapis/js-genai

      // --- Configuration ---
      let API_KEY = "AIzaSyDwucNXYTYj_Za2IFZL3PQYcatbDUErGY8";
      // you can hardcode an API key here if you want to serve just this file from localhost

      const MODEL_NAME = "gemini-2.0-flash-live-001"; // Realtime/Live model
      const TARGET_SAMPLE_RATE = 16000; // Gemini requires 16kHz audio
      const SYSTEM_PROMPT =
        "Please be a helpful assistant and kind conversationalist. " +
        "Respond to the user(s) directly by answering with relevant information " +
        "in full detail. When you do not have relevant information, use your " +
        "Google Search tool to search for it. You can use Markdown and LaTeX. " +
        "Do not merely describe the user(s)' utterances. Do not include timestamps.";
      const INSTRUCTIONS =
        "**Instructions:**\n\nYou can interact with Gemini Live " +
        "by speaking, uploading image files or from your camera, or typing or " +
        "pasting text. You can search the web, ask for Python code to be executed, " +
        "and see images like graphs from Matplotlib, as well as LaTeX mathematics " +
        'typesetting.\n\nTry asking, "What can you do?"\n\n';

      // --- DOM Elements ---
      const controls = document.getElementById("controls");
      const toggleButton = document.getElementById("toggleStream");
      const outputArea = document.getElementById("output");
      const sendImageButton = document.getElementById("sendImage");
      const sendTextButton = document.getElementById("sendText");
      const debugContainer = document.getElementById("debugContainer");
      const floatingTextInput = document.getElementById("floatingTextInput");
      const textInput = document.getElementById("textInput");

      // --- State Variables ---
      let genAI;
      let liveSession = null;
      let mediaStream = null;
      let audioContext = null;
      let audioSource = null;
      let audioWorkletNode = null;
      let isStreaming = false;
      let currentTurnText = "";
      let totalBytesSent = 0;
      let contextTurns = [];

      function getCookie(name) {
        const value = `; ${document.cookie}`;
        const parts = value.split(`; ${name}=`);
        if (parts.length === 2) return parts.pop().split(";").shift();
      }

      function checkApiKey() {
        if (API_KEY) {
          return true;
        }
        API_KEY = getCookie("gemini_api_key");
        if (!API_KEY) {
          liveOutput(
            "API Key not set. Please reload this page to enter it.",
            "error",
          );
          return false;
        }
        return true;
      }

      // --- Debug Checkbox Logic ---
      const debugCheckbox = document.getElementById("debugCheckbox");
      let debugMode = false; // Flag to control debug logging
      // Add an event listener to the checkbox to toggle debug mode
      debugCheckbox.addEventListener("change", () => {
        debugMode = debugCheckbox.checked;
      });

      marked.use(markedKatex({ throwOnError: false })); // LaTeX

      function liveOutput(text, type = "text") {
        const line = document.createElement("div");
        line.innerHTML = marked
          .parse(text)
          .replace("<a href=", '<a TARGET="_blank" href='); // all links open in new tab
        line.classList.add("output");
        if (type === "error") {
          line.classList.add("error");
          console.error(text);
        } else if (type === "info") {
          line.classList.add("info");
          if (debugMode) {
            console.log(text);
          }
        } else {
          line.classList.add("text");
        }
        if (type !== "error" && type !== "info") {
          // Only log "Gemini:" if not error or info
          if (debugMode) {
            console.log("Gemini:", text.replace("\n&nbsp;", ""));
          }
        }
        outputArea.appendChild(line);
        outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
      }

      function arrayBufferToBase64(buffer) {
        let binary = "";
        const bytes = new Uint8Array(buffer);
        const len = bytes.byteLength;
        for (let i = 0; i < len; i++) {
          binary += String.fromCharCode(bytes[i]);
        }
        return window.btoa(binary);
      }

      // --- Audio Worklet ---
      const AudioRecordingWorklet = `
            class AudioProcessingWorklet extends AudioWorkletProcessor {
                buffer = new Int16Array(2048);
                bufferWriteIndex = 0;

                constructor() {
                    super();
                }

                process(inputs, outputs, parameters) {
                    if (inputs.length > 0 && inputs[0].length > 0) {
                        const channelData = inputs[0][0];
                        this.processChunk(channelData);
                    }
                    return true; // Keep processor alive
                }

                sendAndClearBuffer() {
                    if (this.bufferWriteIndex > 0) {
                        const dataToSend = this.buffer.slice(0, this.bufferWriteIndex);
                        this.port.postMessage({
                            eventType: "audioData",
                            audioData: dataToSend.buffer // Send ArrayBuffer
                        }, [dataToSend.buffer]); // Transfer buffer ownership for efficiency
                        this.bufferWriteIndex = 0;
                    }
                }

                processChunk(float32Array) {
                    for (let i = 0; i < float32Array.length; i++) {
                        const clampedValue = Math.max(-1.0, Math.min(1.0, float32Array[i]));
                        const int16Value = Math.floor(clampedValue * 32767);
                        this.buffer[this.bufferWriteIndex++] = int16Value;
                        if (this.bufferWriteIndex >= this.buffer.length) {
                            this.sendAndClearBuffer();
                        }
                    }
                }
            }
            registerProcessor('audio-processing-worklet', AudioProcessingWorklet);
        `;

      // --- Volume Meter Worklet --
      const VolumeMeterWorklet = `
        class VolumeMeter extends AudioWorkletProcessor {
            constructor() {
                super();
                this.volume = 0;
            }
            
            process(inputs) {
                const input = inputs[0];
                if (input.length > 0 && input[0].length > 0) {
                    // Calculate RMS (Root Mean Square)
                    let sumOfSquares = 0.0;
                    for (const sample of input[0]) {
                        sumOfSquares += sample * sample;
                    }
                    const rms = Math.sqrt(sumOfSquares / input[0].length);
                    
                    // Convert RMS to a linear scale (0.0 to 1.0)
                    this.volume = Math.min(1.0, rms * 10); // Adjust multiplier as needed for sensitivity
                    // Post a message to main thread with the volume level
                    this.port.postMessage({ volume: this.volume });
                } else {
                    this.volume = 0;
                }
                
                return true;
            }
        }
        registerProcessor('volume-meter', VolumeMeter);
        `;
      let volumeWorkletNode;

      // Function to update the VU meter's background color
      function updateVuMeter(volume) {
        const darkness = Math.round(volume * 150); // Adjust multiplier for sensitivity
        const newRed = Math.max(0, 224 - darkness); // Base red: 224
        const newGreen = Math.max(0, 255 - darkness); // Base green: 255
        const newBlue = Math.max(0, 255 - darkness); // Base blue: 255
        controls.style.backgroundColor = `rgb(${newRed}, ${newGreen}, ${newBlue})`;
      }

      // --- Core Streaming Logic ---

      async function startStreaming() {
        if (isStreaming) return;
        if (!checkApiKey()) {
          return;
        }

        isStreaming = true; // Set streaming flag early
        toggleButton.innerHTML = "Stop<br>Listening";
        toggleButton.classList.add("stop");

        try {
          // Step 1: Initialize GoogleGenAI Client
          genAI = new GoogleGenAI({ apiKey: API_KEY });
          try {
            const modelInfo = await genAI.models.get({ model: MODEL_NAME });
            if (debugMode) {
              console.log("Gemini model info:", modelInfo);
            }
          } catch (testError) {
            // Clear the cookie
            document.cookie =
              "gemini_api_key=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;";
            // Redirect to root
            window.location.href = "/";
            return;
          }

          // Step 2: Get Microphone Access
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              sampleRate: TARGET_SAMPLE_RATE,
              echoCancellation: false,
              noiseSuppression: true,
            },
          });

          // Step 3: Create Audio Context and Source
          audioContext = new AudioContext({ sampleRate: TARGET_SAMPLE_RATE });
          // Resume context if suspended (often needed after page load)
          if (audioContext.state === "suspended") {
            await audioContext.resume();
          }
          audioSource = audioContext.createMediaStreamSource(mediaStream);
          // Create a gain node to split the audio signal
          const splitter = audioContext.createChannelSplitter(2);
          const merger = audioContext.createChannelMerger(2);
          const gainNode = audioContext.createGain();
          gainNode.gain.value = 1; // No gain change

          // Step 4: Set up Audio Worklet
          const workletBlob = new Blob([AudioRecordingWorklet], {
            type: "application/javascript",
          });
          const workletURL = URL.createObjectURL(workletBlob);
          try {
            await audioContext.audioWorklet.addModule(workletURL);
          } catch (e) {
            liveOutput(
              `Error adding AudioWorklet module: ${e.message}. Make sure you are serving this page over HTTPS or localhost.`,
              "error",
            );
            throw e; // Re-throw to be caught by outer catch
          }
          audioWorkletNode = new AudioWorkletNode(
            audioContext,
            "audio-processing-worklet",
          );

          // Add the volume meter worklet
          const volumeWorkletBlob = new Blob([VolumeMeterWorklet], {
            type: "application/javascript",
          });
          const volumeWorkletURL = URL.createObjectURL(volumeWorkletBlob);
          try {
            await audioContext.audioWorklet.addModule(volumeWorkletURL);
          } catch (e) {
            liveOutput(
              `Error adding VolumeMeter AudioWorklet module: ${e.message}. Make sure you are serving this page over HTTPS or localhost.`,
              "error",
            );
            throw e; // Re-throw to be caught by outer catch
          }
          volumeWorkletNode = new AudioWorkletNode(
            audioContext,
            "volume-meter",
          );

          // Connect audio nodes: Mic Source -> Gain Node -> Volume Worklet
          audioSource.connect(gainNode);
          gainNode.connect(volumeWorkletNode);

          //connect audio nodes: Gain Node -> split -> merge -> Gemini Worklet
          gainNode.connect(splitter);
          splitter.connect(merger, 0, 0); // Connect channel 0 to input 0
          splitter.connect(merger, 0, 1); // Connect channel 0 to input 1
          merger.connect(audioWorkletNode);

          volumeWorkletNode.connect(audioWorkletNode);

          // Step 5: Connect to Gemini Live API
          // Assign to liveSession *after* connection is successful

          liveSession = await genAI.live.connect({
            model: MODEL_NAME,
            audioConfig: { targetSampleRate: TARGET_SAMPLE_RATE },
            systemInstruction: SYSTEM_PROMPT,
            inputAudioTranscription: {}, // Enable audio transcription, not working; being worked on now
            config: {
              responseModalities: [Modality.TEXT],
              tools: [{ codeExecution: {} }, { googleSearch: {} }],
              temperature: 0,
              seed: 42,
            },
            callbacks: {
              onopen: () => {
                // The connection to Gemini is open; audio processing and sending can begin
                debugContainer.style.display = "none"; // Hide controls
                sendImageButton.style.display = "inline-block"; // Show the button
                sendTextButton.style.display = "inline-block";
                //liveOutput("Connected to Gemini. Listening...", 'info');
              },
              onmessage: (message) => {
                if (debugMode) {
                  console.log("Debug message:", message);
                }

                if (message.serverContent?.modelTurn?.parts?.length > 0) {
                  const part = message.serverContent.modelTurn.parts[0];

                  if (
                    part.inlineData &&
                    part.inlineData.mimeType.startsWith("image/")
                  ) {
                    try {
                      // Handle image data
                      const img = document.createElement("img");
                      img.classList.add("output-image"); // Add CSS class for styling
                      img.src = `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
                      liveOutput("Image from Gemini:", "info");
                      outputArea.appendChild(img);
                      outputArea.parentNode.scrollTop =
                        outputArea.parentNode.scrollHeight;
                      // append model turn to context
                      if (!part.inlineData.mimeType.startsWith("image/gif")) {
                        // .gif files cause bugs
                        contextTurns.push({
                          role: "model",
                          parts: [
                            {
                              inlineData: {
                                mimeType: part.inlineData.mimeType,
                                data: part.inlineData.data,
                              },
                            },
                          ],
                        });
                      }
                    } catch (renderError) {
                      liveOutput(
                        `Error rendering image: ${renderError.message}`,
                        "error",
                      );
                    }
                  } else if (part.executableCode) {
                    liveOutput(
                      "**Python code:**\n```\n" +
                        part.executableCode.code +
                        " ```",
                    );
                    // append model turn to context
                    contextTurns.push({
                      role: "model",
                      parts: [{ text: part.executableCode.code }],
                    });
                  } else if (part.codeExecutionResult) {
                    const result = part.codeExecutionResult;
                    liveOutput(
                      "**Output:** (" +
                        result.outcome +
                        ")\n``` \n" +
                        result.output +
                        " ```",
                    );
                    // append model turn to context
                    contextTurns.push({
                      role: "model",
                      parts: [
                        { text: result.outcome + "\n\n" + result.output },
                      ],
                    });
                  } else {
                    const textPart = message.serverContent.modelTurn.parts.find(
                      (part) => part.text,
                    );
                    if (textPart && textPart.text.trim()) {
                      currentTurnText += textPart.text;
                    }
                  }
                }
                if (
                  message.serverContent?.turnComplete ||
                  message.generationComplete
                ) {
                  // Finalize text when a turn was completed
                  if (currentTurnText.trim().length > 0) {
                    liveOutput(currentTurnText + "\n&nbsp;");
                    // append model turn to context
                    contextTurns.push({
                      role: "model",
                      parts: [{ text: currentTurnText }],
                    });
                  }
                  currentTurnText = ""; // Reset for next turn
                } else if (message.inputTranscription) {
                  liveOutput("You said: " + message.inputTranscription, "info");
                }

                if (message.error) {
                  liveOutput(
                    `Gemini Error: ${
                      message.error.message || JSON.stringify(message.error)
                    }`,
                    "error",
                  );
                }
              },
              onerror: (errorEvent) => {
                // Use errorEvent directly if it's an Error object, otherwise stringify
                const errorMessage =
                  errorEvent instanceof Error
                    ? errorEvent.message
                    : JSON.stringify(errorEvent);
                liveOutput(`Gemini connection error: ${errorMessage}`, "error");
                stopStreaming(); // Stop on connection error
              },
              onclose: (closeEvent) => {
                // Only call stopStreaming if the closure was unexpected while we were actively streaming
                if (isStreaming) {
                  liveOutput("Connection closed unexpectedly.", "error");
                  stopStreaming();
                }
              },
            },
          });
          // send prior context if any
          if (contextTurns.length) {
            liveOutput(
              `Sending ${contextTurns.length} turns of prior context above.`,
              "info",
            );
            contextTurns.push({
              role: "user",
              parts: [{ text: "Please summarize our discussion so far." }],
            });
            await liveSession.sendClientContent({
              turns: contextTurns,
              turnComplete: true,
            });
          }
          liveOutput("Connected to Gemini. Listening...", "info");

          // Step 6: Handle Audio Data from Worklet
          audioWorkletNode.port.onmessage = (event) => {
            // Check if it's audio data, if the session exists, and if we are still streaming
            if (
              event.data.eventType === "audioData" &&
              liveSession &&
              isStreaming
            ) {
              const audioDataBuffer = event.data.audioData;
              const base64AudioData = arrayBufferToBase64(audioDataBuffer);

              try {
                // Send audio chunk to Gemini
                liveSession.sendRealtimeInput({
                  media: {
                    data: base64AudioData,
                    mimeType: `audio/pcm;rate=${TARGET_SAMPLE_RATE}`,
                  },
                });
                totalBytesSent += audioDataBuffer.byteLength;
                if (debugMode && (totalBytesSent - 4096) % 163840 === 0) {
                  // every ~5 seconds
                  console.log(
                    "Audio sent",
                    totalBytesSent,
                    "bytes,",
                    Math.round(totalBytesSent / 32000),
                    "seconds",
                  );
                }
              } catch (sendError) {
                liveOutput(
                  `Error sending audio data: ${sendError.message}`,
                  "error",
                );
                // Optionally stop streaming if sending fails repeatedly
                // stopStreaming();
              }
            }
          };

          // Handle volume events
          volumeWorkletNode.port.onmessage = (event) => {
            if (event.data.volume !== undefined) {
              updateVuMeter(event.data.volume);
            }
          };
        } catch (error) {
          liveOutput(
            `Error starting stream: ${error.message || error}`,
            "error",
          );
          await stopStreaming(); // Ensure cleanup happens even on startup error
        }
      }

      async function stopStreaming() {
        // Prevent multiple stop calls overlapping
        if (!isStreaming && !liveSession && !mediaStream && !audioContext) {
          console.log("Stop called but already stopped/cleaned up.");
          return;
        }
        const wasStreaming = isStreaming; // Keep track if we were actively streaming
        isStreaming = false; // Set flag immediately to stop sending data

        // Step 7: Close Gemini Session
        if (liveSession) {
          try {
            liveSession.close();
          } catch (e) {
            liveOutput(`Error closing Gemini session: ${e.message}`, "error");
          } finally {
            liveSession = null;
          }
        }

        // Step 8: Stop Audio Processing
        if (audioWorkletNode) {
          audioWorkletNode.disconnect();
          audioWorkletNode = null;
        }
        // disconnect the volume meter node
        if (volumeWorkletNode) {
          volumeWorkletNode.disconnect();

          volumeWorkletNode = null;
        }
        if (audioSource) {
          audioSource.disconnect();
          audioSource = null;
        }
        // Step 9: Stop Media Stream Tracks
        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
          mediaStream = null;
        }
        // Step 10: Close Audio Context
        if (audioContext && audioContext.state !== "closed") {
          try {
            await audioContext.close();
          } catch (e) {
            liveOutput(`Error closing AudioContext: ${e.message}`, "error");
          } finally {
            audioContext = null;
          }
        }

        // Step 11: Reset UI
        toggleButton.innerHTML = "Start<br>Listening";
        toggleButton.classList.remove("stop");
        sendTextButton.style.display = "none"; // Hide the text button
        sendImageButton.style.display = "none"; // Hide the image button
        debugContainer.style.display = "inline-block"; // Restore controls
        genAI = null; // Clear the client instance
        if (wasStreaming) {
          liveOutput("Stopped listening.\n", "info");
        }
      }

      // --- Event Listener ---
      toggleButton.addEventListener("click", () => {
        if (isStreaming) {
          stopStreaming();
        } else {
          startStreaming();
        }
      });

      // "Send Image" button event listener
      document.getElementById("sendImage").addEventListener("click", () => {
        // Create a file input element
        const input = document.createElement("input");
        input.type = "file";
        input.accept = "image/*"; // Accept only image files

        // Add event listener for file selection
        input.addEventListener("change", (event) => {
          const file = event.target.files[0];
          if (file) {
            liveOutput(`Processing image: ${file.name}`, "info");
            const reader = new FileReader();
            reader.onload = async () => {
              try {
                let base64Data = reader.result.split(",")[1];
                let mimeType = file.type;

                // Downscale if necessary
                if (base64Data.length > 250 * 1024 * 0.75) {
                  // Approximate base64 size
                  const downscaled = await downscaleImage(
                    base64Data,
                    mimeType,
                    250 * 1024,
                  );
                  base64Data = downscaled.base64;
                  mimeType = downscaled.mimeType;
                  liveOutput(
                    `Image downscaled to ${(
                      (base64Data.length * 0.75) /
                      1024
                    ).toFixed(2)}KB`,
                    "info",
                  );
                }

                // Display image in output area
                const img = document.createElement("img");
                img.classList.add("output-image");
                img.src = `data:${mimeType};base64,${base64Data}`;
                outputArea.appendChild(img);
                outputArea.parentNode.scrollTop =
                  outputArea.parentNode.scrollHeight;

                // Send image to stream
                sendImageToStream(base64Data, mimeType);
              } catch (error) {
                liveOutput(`Error processing image: ${error.message}`, "error");
              }
            };
            reader.onerror = (error) => {
              liveOutput(`Failed to read file: ${error}`, "error");
            };
            reader.readAsDataURL(file);
          }
        });

        // Trigger the file input dialog
        input.click();
      });

      // Helper function for downscaling
      function downscaleImage(base64, mimeType, maxSize) {
        return new Promise((resolve, reject) => {
          const img = new Image();
          img.onload = () => {
            let width = img.width;
            let height = img.height;
            let newWidth = width;
            let newHeight = height;

            // Calculate scaling factor to fit within maxSize
            const canvas = document.createElement("canvas");
            const ctx = canvas.getContext("2d");

            // Binary search for the right quality
            let low = 0.1;
            let high = 1.0;
            let quality = 0.8; // Initial guess
            let result = null;

            for (let i = 0; i < 10; i++) {
              // 10 iterations of binary search
              canvas.width = newWidth;
              canvas.height = newHeight;
              ctx.drawImage(img, 0, 0, newWidth, newHeight);
              const newBase64 = canvas
                .toDataURL(mimeType, quality)
                .split(",")[1];
              const newSize = newBase64.length * 0.75;

              if (newSize <= maxSize) {
                result = { base64: newBase64, mimeType: mimeType };
                low = quality; // Try for better quality
              } else {
                high = quality;
                // Scale down image dimensions to reduce size.
                const scaleFactor = Math.sqrt(maxSize / newSize);
                newWidth = Math.floor(newWidth * scaleFactor);
                newHeight = Math.floor(newHeight * scaleFactor);
              }
              quality = (low + high) / 2;
            }

            if (result) {
              resolve(result);
            } else {
              // Should not happen after scaling image dimensions
              reject(
                new Error("Could not downscale image to the required size."),
              );
            }
          };
          img.onerror = () =>
            reject(new Error("Failed to load image for downscaling."));
          img.src = `data:${mimeType};base64,${base64}`;
        });
      }

      // Function to send the image to the stream
      async function sendImageToStream(base64Image, mimeType = "image/jpeg") {
        if (!liveSession || !isStreaming) {
          liveOutput(
            "Error: Live session not active. Please start listening first.",
            "error",
          );
          return;
        }
        try {
          // Debug information
          if (debugMode) {
            console.log(
              `Sending image (${((base64Image.length * 0.75) / 1024).toFixed(
                2,
              )}KB) with MIME type: ${mimeType}`,
            );
          }

          // Send the image as a completed turn
          await liveSession.sendClientContent({
            turns: [
              {
                role: "user",
                parts: [
                  {
                    inlineData: {
                      mimeType: mimeType,
                      data: base64Image,
                    },
                  },
                ],
              },
            ],
            turnComplete: true,
          });
          liveOutput(
            "Image sent successfully! Waiting for Gemini to process...",
            "info",
          );
          // append user turn to context
          contextTurns.push({
            role: "user",
            parts: [{ inlineData: { mimeType: mimeType, data: base64Image } }],
          });
        } catch (error) {
          // Handle the error appropriately
          liveOutput(
            `Error sending image: ${error.message || JSON.stringify(error)}`,
            "error",
          );
        }
      }

      // Function to submit text input from the floating input area
      function submitTextInput() {
        const text = textInput.value;
        floatingTextInput.style.display = "none";
        textInput.value = "";
        if (text.trim() !== "") {
          // Check for empty input
          sendTextToStream(text); // Send the text to the stream
        }
      }

      // If using Enter key in the textarea:
      textInput.addEventListener("keydown", (event) => {
        if (event.key === "Enter" && !event.shiftKey) {
          // Enter without Shift
          event.preventDefault(); // Prevent default Enter behavior (newline)
          submitTextInput();
        }
      });

      async function sendTextToStream(text) {
        if (!liveSession || !isStreaming) {
          liveOutput(
            "Error: Live session not active. Please start listening first.",
            "error",
          );
          return;
        }
        try {
          await liveSession.sendClientContent({
            turns: [
              {
                role: "user",
                parts: [
                  {
                    text: text,
                  },
                ],
              },
            ],
            turnComplete: true,
          });
          liveOutput("Sent: " + text, "info");
          // append user turn to context
          contextTurns.push({ role: "user", parts: [{ text: text }] });
        } catch (error) {
          liveOutput(
            `Error sending text: ${error.message || JSON.stringify(error)}`,
            "error",
          );
        }
      }

      // Event listener for the Send button in the floating text input
      document
        .querySelector("#floatingTextInput button")
        .addEventListener("click", () => {
          submitTextInput();
        });

      // Add a click event listener to the sendTextButton
      sendTextButton.addEventListener("click", () => {
        floatingTextInput.style.display = "block"; // Display the input area
        textInput.focus(); // Focus on the text input area
      });

      // Event listener to hide floatingTextInput on outside click
      document.addEventListener("click", (event) => {
        if (
          floatingTextInput.style.display === "block" &&
          !floatingTextInput.contains(event.target) &&
          event.target !== sendTextButton // Check if the click is NOT on the sendTextButton
        ) {
          floatingTextInput.style.display = "none";
        }
      });

      if (checkApiKey()) {
        const instructions = document.createElement("div");
        instructions.innerHTML = marked.parse(INSTRUCTIONS);
        outputArea.appendChild(instructions);
        outputArea.parentNode.scrollTop = outputArea.parentNode.scrollHeight;
        liveOutput("Click 'Start Listening' to begin.", "info");
      }

      // Add safety net for page unload
      window.addEventListener("beforeunload", () => {
        if (isStreaming) {
          stopStreaming(); // Attempt cleanup if user navigates away
        }
      });
    </script>
  </body>
</html>
